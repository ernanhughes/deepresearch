{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import sqlite3\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "# Configuration Class\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.search_query = \"agent\"\n",
    "        self.max_results = 50\n",
    "        self.output_folder = \"data\"\n",
    "        self.base_url = \"http://export.arxiv.org/api/query?\"\n",
    "        self.log_db_path = \"app_logs.db\"\n",
    "\n",
    "    def load_from_env(self):\n",
    "        self.search_query = os.environ.get(\"SEARCH_QUERY\", self.search_query)\n",
    "        self.max_results = int(os.environ.get(\"MAX_RESULTS\", self.max_results))\n",
    "        self.output_folder = os.environ.get(\"OUTPUT_FOLDER\", self.output_folder)\n",
    "        self.base_url = os.environ.get(\"BASE_URL\", self.base_url)\n",
    "        self.log_db_path = os.environ.get(\"LOG_DB_PATH\", self.log_db_path)\n",
    "        return self\n",
    "\n",
    "    # Optional: Load from a file (e.g. JSON, YAML) - Implement as needed\n",
    "    # def load_from_file(self, config_file: str):\n",
    "    #     pass # Implement file loading logic\n",
    "\n",
    "\n",
    "# Initialize Configuration\n",
    "config = Config().load_from_env()  # Load from env variables first\n",
    "\n",
    "# DatabaseHandler for logging\n",
    "class DatabaseHandler(logging.Handler):\n",
    "    def __init__(self, db_path):\n",
    "        super().__init__()\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            if self.conn is None or self.conn.closed:\n",
    "                self.conn = sqlite3.connect(self.db_path)\n",
    "                self.conn.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS logs (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        timestamp TEXT,\n",
    "                        level TEXT,\n",
    "                        message TEXT\n",
    "                    )\n",
    "                \"\"\")\n",
    "                self.conn.commit()\n",
    "\n",
    "            cursor = self.conn.cursor()\n",
    "            import datetime\n",
    "            timestamp = datetime.datetime.fromtimestamp(record.created).isoformat()\n",
    "            log_entry = (timestamp, record.levelname, record.getMessage())\n",
    "            cursor.execute(\"INSERT INTO logs (timestamp, level, message) VALUES (?, ?, ?)\", log_entry)\n",
    "            self.conn.commit()\n",
    "\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error logging to database: {e}\")\n",
    "            if self.conn:\n",
    "                self.conn.rollback()\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error in DatabaseHandler: {e}\")\n",
    "            if self.conn:\n",
    "                self.conn.rollback()\n",
    "        finally:\n",
    "            if self.conn:\n",
    "                self.conn.close()\n",
    "\n",
    "# Configure logging to console, file, and database\n",
    "log_file_path = \"app.log\"\n",
    "file_handler = logging.FileHandler(log_file_path)\n",
    "db_handler = DatabaseHandler(config.log_db_path)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "db_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(db_handler)\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_arxiv_papers(search_query: str = None, max_results: Optional[int] = None) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Searches arXiv, downloads papers, and adds metadata to a database.\"\"\"\n",
    "\n",
    "    search_query = search_query or config.search_query\n",
    "    max_results = max_results or config.max_results\n",
    "\n",
    "    db_name = f\"{search_query}.db\"\n",
    "    conn = None\n",
    "\n",
    "    try:\n",
    "        Path(config.output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS papers (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                search_query TEXT,\n",
    "                title TEXT UNIQUE,\n",
    "                pdf_link TEXT,\n",
    "                file_path TEXT,\n",
    "                file_hash TEXT,\n",
    "                file_content BLOB\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "\n",
    "        logger.info(f\"Searching for papers on '{search_query}'...\")\n",
    "        response_text = _fetch_arxiv_metadata(search_query, max_results)\n",
    "        papers = _parse_paper_links(response_text)\n",
    "\n",
    "        logger.info(f\"Found {len(papers)} papers. Starting download...\")\n",
    "        downloaded_count = 0\n",
    "        for title, pdf_link in papers:\n",
    "            try:\n",
    "                file_path = _download_paper(title, pdf_link, config.output_folder)\n",
    "                if file_path:\n",
    "                    file_hash = compute_file_hash(file_path)\n",
    "\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        file_content = f.read()\n",
    "\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT OR IGNORE INTO papers (search_query, title, pdf_link, file_path, file_hash, file_content)\n",
    "                        VALUES (?, ?, ?, ?, ?, ?)\n",
    "                    \"\"\", (search_query, title, pdf_link, file_path, file_hash, file_content))\n",
    "                    conn.commit()\n",
    "\n",
    "                    downloaded_count += 1\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping database entry for {title} due to download failure.\")\n",
    "\n",
    "            except sqlite3.Error as e:\n",
    "                logger.error(f\"Database error: {e}\")\n",
    "                if conn:\n",
    "                    conn.rollback()\n",
    "                return []\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"An unexpected error occurred during processing of {title}: \")\n",
    "                if conn:\n",
    "                    conn.rollback()\n",
    "                return []\n",
    "\n",
    "        logger.info(f\"Download and database update complete! {downloaded_count} papers processed.\")\n",
    "        return papers\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        logger.error(f\"Database connection error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.exception(\"A general error occurred:\")\n",
    "        return []\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def sanitize_filename(title: str) -> str:\n",
    "    return re.sub(r\"[^\\w\\s-]\", \"\", title).strip().replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "def _get_filename_from_url(url: str) -> str:\n",
    "    parsed_url = urlparse(url)\n",
    "    return os.path.basename(parsed_url.path).split(\".\")[0]\n",
    "\n",
    "\n",
    "def compute_file_hash(file_path: str, algorithm: str = \"sha256\") -> str:\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        for chunk in iter(lambda: file.read(8192), b\"\"):\n",
    "            hash_func.update(chunk)\n",
    "    return hash_func.hexdigest()\n",
    "\n",
    "\n",
    "def _fetch_arxiv_metadata(search_query: str, max_results: int) -> str:\n",
    "    url = f\"{config.base_url}search_query=all:{search_query}&start=0&max_results={max_results}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def _parse_paper_links(response_text: str) -> List[Tuple[str, str]]:\n",
    "    root = ET.fromstring(response_text)\n",
    "    papers = []\n",
    "    for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "        pdf_link = None\n",
    "        title = None\n",
    "        for link in entry.findall(\"{http://www.w3.org/2005/Atom}link\"):\n",
    "            if link.attrib.get(\"title\") == \"pdf\":\n",
    "                pdf_link = link.attrib[\"href\"] + \".pdf\"\n",
    "                title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "                break\n",
    "\n",
    "        if pdf_link and title:\n",
    "            papers.append((title, pdf_link))\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
