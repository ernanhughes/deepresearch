{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for papers on 'Cellular Automata'...\n",
      "1105.5335v1.pdf\n",
      "1003.1983v1.pdf\n",
      "0702046v1.pdf\n",
      "1004.1830v1.pdf\n",
      "0502061v1.pdf\n",
      "Found 5 papers. Starting download...\n",
      "Downloaded: 1105.5335v1.pdf\n",
      "Downloaded: 1003.1983v1.pdf\n",
      "Downloaded: 0702046v1.pdf\n",
      "Downloaded: 1004.1830v1.pdf\n",
      "Downloaded: 0502061v1.pdf\n",
      "Download complete!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from smolagents import CodeAgent, ToolCallingAgent, DuckDuckGoSearchTool, LiteLLMModel, PythonInterpreterTool, tool\n",
    "\n",
    "from typing import Optional\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "SEARCH_QUERY= \"agent\"  # Replace with desired search term or topic\n",
    "MAX_RESULTS= 50  # Adjust the number of papers you want to download\n",
    "OUTPUT_FOLDER= \"data\"  # Folder to store downloaded papers\n",
    "BASE_URL= \"http://export.arxiv.org/api/query?\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_ariv_papers(search_query:str, max_results: Optional[int]=5)->str:\n",
    "    \"\"\"\n",
    "    Searches ariv for research papers on a topic and saved teh papers to a folder\n",
    "    Args:\n",
    "        search_query: the topic to search for\n",
    "        max_results: max results to return\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "    # Fetch and parse papers\n",
    "    print(f\"Searching for papers on '{search_query}'...\")\n",
    "    response_text = fetch_arxiv_papers(search_query, max_results)\n",
    "    papers = parse_paper_links(response_text)\n",
    "\n",
    "    # Download each paper\n",
    "    print(f\"Found {len(papers)} papers. Starting download...\")\n",
    "    for title, pdf_link in papers:\n",
    "        try:\n",
    "            download_paper(title, pdf_link, OUTPUT_FOLDER)\n",
    "            time.sleep(2)  # Pause to avoid hitting rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download '{title}': {e}\")\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "\n",
    "\n",
    "def sanitize_filename(title):\n",
    "    \"\"\"Sanitizes a string to be used as a filename.\"\"\"\n",
    "    # Remove any characters that are not alphanumeric, spaces, hyphens, or underscores\n",
    "    return re.sub(r\"[^\\w\\s-]\", \"\", title).strip().replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "def get_filename_from_url(url):\n",
    "    # Parse the URL to get the path component\n",
    "    parsed_url = urlparse(url)\n",
    "    # Get the base name from the URL's path\n",
    "    filename = os.path.basename(parsed_url.path)\n",
    "    return filename\n",
    "\n",
    "def compute_file_hash(file_path, algorithm=\"sha256\"):\n",
    "    \"\"\"Compute the hash of a file using the specified algorithm.\"\"\"\n",
    "    hash_func = hashlib.new(algorithm)\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # Read the file in chunks of 8192 bytes\n",
    "        while chunk := file.read(8192):\n",
    "            hash_func.update(chunk)\n",
    "\n",
    "    return hash_func.hexdigest()\n",
    "\n",
    "def fetch_arxiv_papers(search_query, max_results=5):\n",
    "    \"\"\"Fetches metadata of papers from arXiv using the API.\"\"\"\n",
    "    url = f\"{BASE_URL}search_query=all:{search_query}&start=0&max_results={max_results}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def parse_paper_links(response_text):\n",
    "    \"\"\"Parses paper links and titles from arXiv API response XML.\"\"\"\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    root = ET.fromstring(response_text)\n",
    "    papers = []\n",
    "    for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "        pdf_link = None\n",
    "        for link in entry.findall(\"{http://www.w3.org/2005/Atom}link\"):\n",
    "            if link.attrib.get(\"title\") == \"pdf\":\n",
    "                pdf_link = link.attrib[\"href\"] + \".pdf\"\n",
    "                break\n",
    "        if pdf_link:\n",
    "            title = get_filename_from_url(pdf_link)\n",
    "            print(title)\n",
    "            papers.append((title, pdf_link))\n",
    "    return papers\n",
    "\n",
    "\n",
    "def download_paper(title, pdf_link, output_folder):\n",
    "    \"\"\"Downloads a single paper PDF.\"\"\"\n",
    "    # Create a safe filename\n",
    "    safe_title = sanitize_filename(title)\n",
    "    filename = os.path.join(output_folder, f\"{safe_title}.pdf\")\n",
    "    response = requests.get(pdf_link, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Write the PDF to the specified folder\n",
    "    with open(filename, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "    print(f\"Downloaded: {title}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = fetch_ariv_papers(search_query=\"Cellular Automata\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "def extract_markdown_from_notebook(notebook_path, output_path=None):\n",
    "    with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    markdown_cells = [\n",
    "        cell[\"source\"] for cell in notebook[\"cells\"] if cell[\"cell_type\"] == \"markdown\"\n",
    "    ]\n",
    "\n",
    "    markdown_text = \"\\n\\n\".join(\"\".join(cell) for cell in markdown_cells)\n",
    "\n",
    "    if output_path:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_text)\n",
    "    \n",
    "    return markdown_text\n",
    "\n",
    "# Example usage\n",
    "notebook_file = \"example.ipynb\"  # Change this to your notebook file\n",
    "output_file = \"extracted_markdown.md\"  # Change this if you want to save it\n",
    "\n",
    "markdown_content = extract_markdown_from_notebook(notebook_file, output_file)\n",
    "print(markdown_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown extracted and saved as 'extracted_markdown.md'\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "\n",
    "notebook_path = \"paper_retriever_tool.ipynb\"\n",
    "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "markdown_cells = [\n",
    "    \"\".join(cell[\"source\"]) for cell in notebook.cells if cell.cell_type == \"markdown\"\n",
    "]\n",
    "\n",
    "markdown_text = \"\\n\\n\".join(markdown_cells)\n",
    "\n",
    "# Save to file\n",
    "with open(\"paper_retriever_tool.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown_text)\n",
    "\n",
    "print(\"Markdown extracted and saved as 'extracted_markdown.md'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
